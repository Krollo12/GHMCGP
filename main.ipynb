{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "from tqdm.notebook import tqdm\n",
    "from pyro.contrib.gp.likelihoods.likelihood import Likelihood\n",
    "from pyro.nn.module import PyroParam\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions.torch_distribution import TorchDistributionMixin\n",
    "from torch.distributions.utils import _standard_normal, broadcast_all\n",
    "from torch.distributions.exp_family import ExponentialFamily\n",
    "from pyro.contrib.gp.parameterized import Parameterized\n",
    "from numbers import Number\n",
    "from torch.distributions import constraints\n",
    "from pyro.infer.autoguide import AutoMultivariateNormal, AutoDiagonalNormal,AutoLowRankMultivariateNormal\n",
    "from pyro.infer import SVI, EmpiricalMarginal, TracePosterior, Trace_ELBO, Predictive,TraceMeanField_ELBO,TraceGraph_ELBO,TracePredictive\n",
    "\n",
    "from GP_models import GPModel, VariationalGP, VariationalMGP, HGPModel, VariationalMHGP,VariationalPoisGP\n",
    "from GP_likelihoods import PyroCensoredNormal, CensoredHomoscedGaussian, HomoscedGaussian, CensoredHeteroGaussian, PyroCensoredPois, Censored_Poisson, Poisson\n",
    "from utils import create_1d_data, censor_1d_data, create_2nd_data, log_error_f\n",
    "from training_utils import standard_gp_train, censored_gp_train, multi_gp_train, hetero_multi_gp_train, _zero_mean_function\n",
    "\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)  \n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('axes', titlesize=12)\n",
    "plt.rc('axes', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(49)\n",
    "np.random.seed(10)\n",
    "N=100\n",
    "X1_t,Y1_t,y,noise_scale = create_1d_data(N=100)\n",
    "X2,Y2,y2,noise_scale_y2 = create_2nd_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(X1_t,Y1_t,'x',label='Observed')\n",
    "plt.plot(X1_t,y,'--',label='True')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity = [0.2,0.5,0.8]\n",
    "file = open('Experiments/Synthetic/SGP/Stats_synthetic.txt', \"w\")\n",
    "file1 = open('Experiments/Synthetic/CGP/Stats_synthetic.txt', \"w\")\n",
    "file2 = open('Experiments/Synthetic/MGP/Stats_synthetic.txt', \"w\")\n",
    "file3 = open('Experiments/Synthetic/HMGP/Stats_synthetic.txt', \"w\")\n",
    "for intens in intensity:\n",
    "    X1,Y1,y_sc,Y1_cens_sc,censoring = censor_1d_data(X1_dat=X1_t,Y1_dat=Y1_t,y_dat=y,int_low = intens)\n",
    "    standard_gp_train(X1,Y1,Y1_cens_sc,y,y_sc,censoring,intens,noise_scale,file)\n",
    "    censored_gp_train(X1,Y1,Y1_cens_sc,y,y_sc,censoring,intens,noise_scale,file1)\n",
    "    X1 = X1.reshape(-1,1)\n",
    "    X2 = X2.reshape(-1,1)\n",
    "    X1 = X1.type(torch.float64)\n",
    "    Y1_cens_sc = Y1_cens_sc.type(torch.float64)\n",
    "    X2 = X2.type(torch.float64)\n",
    "    Y2 = Y2.type(torch.float64)\n",
    "    X_augmented = np.vstack((np.hstack((X1, np.ones_like(X1))), np.hstack((X2, np.zeros_like(X2)))))\n",
    "    X_augmented = np.hstack((X_augmented,np.vstack((np.zeros_like(X1),np.ones_like(X2)))))\n",
    "    X_augmented = torch.Tensor(X_augmented)\n",
    "    Y_augmented = torch.Tensor((np.vstack((Y1_cens_sc,Y2)))).reshape(-1)\n",
    "    censoring_mul = torch.cat([torch.tensor(censoring),torch.zeros(N).type(torch.int32)])\n",
    "    multi_gp_train(X_augmented,Y_augmented,X1,Y1,X2,Y2,Y1_cens_sc,y,y2,y_sc,censoring,censoring_mul,intens,noise_scale,file2)\n",
    "    hetero_multi_gp_train(X_augmented,Y_augmented,X1,Y1,X2,Y2,Y1_cens_sc,y,y2,y_sc,censoring,censoring_mul,intens,noise_scale,file3)\n",
    "\n",
    "    \n",
    "file.close()\n",
    "file1.close()\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = [0.2,0.5,0.8]\n",
    "\n",
    "for intens in intensities:\n",
    "\n",
    "    N1 = 50\n",
    "    X1 = np.linspace(0, 20, N1)\n",
    "    y = np.round(10*np.sin(0.5*X1)+11)\n",
    "\n",
    "\n",
    "    # Generate noisy observations\n",
    "    noise_scale = torch.linspace(1,1,N1)*1\n",
    "    epsilon = torch.distributions.Normal(loc=0, scale=noise_scale).sample()\n",
    "    Y1 = np.round(y + epsilon.numpy())\n",
    "    Y1_cens = copy.deepcopy(Y1)\n",
    "\n",
    "    censoring = np.int32(10*np.sin(0.5*X1) + 11 >= 17) \n",
    "    p_c = np.random.uniform(low=intens, high=intens+0.1, size=np.sum(censoring==1))\n",
    "    Y1_cens[censoring == 1] = np.round(Y1_cens[censoring == 1]*(1-p_c))\n",
    "    Y1_cens_sc = Y1_cens\n",
    "    y_sc = y\n",
    "\n",
    "    X1 = torch.as_tensor(X1).reshape(-1)\n",
    "    Y1 = torch.as_tensor(Y1).reshape(-1)\n",
    "    y_sc = torch.as_tensor(y_sc).reshape(-1)\n",
    "    Y1_cens_sc = torch.as_tensor(Y1_cens_sc).reshape(-1)\n",
    "\n",
    "    Y1_cens_sc = Y1_cens_sc.type(torch.float64)\n",
    "    X1 = X1.type(torch.float64)\n",
    "    censoring = torch.tensor(censoring).type(torch.float64)\n",
    "    y_sc = y_sc.type(torch.float64)\n",
    "    \n",
    "    print('Running standard Poisson with intensity: {}'.format(intens))\n",
    "    pyro.clear_param_store()\n",
    "    kern = gp.kernels.RBF(input_dim=1,active_dims=[0],lengthscale=torch.tensor(1.),variance=torch.tensor(1.))\n",
    "    kern1 = gp.kernels.Linear(input_dim=1,active_dims=[0],variance=torch.tensor(1.))\n",
    "    full_k = gp.kernels.Sum(kern,kern1)\n",
    "    response_f = log_error_f\n",
    "    like_cens = Poisson(response_function=response_f)\n",
    "    sgphomo = VariationalPoisGP(X=X1, y=Y1_cens_sc, kernel=kern, likelihood=like_cens, mean_function=None,\n",
    "                     latent_shape=None, whiten=False,jitter=0.05)\n",
    "    guide = AutoMultivariateNormal(sgphomo.model)\n",
    "    optimizer = pyro.optim.ClippedAdam({\"lr\": 0.01,\"lrd\": 0.99996})\n",
    "    svi = SVI(sgphomo.model, guide, optimizer, Trace_ELBO(num_particles=10))\n",
    "    num_epochs = 6000\n",
    "    losses = []\n",
    "    pyro.clear_param_store()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        loss = svi.step(X1, Y1_cens_sc)\n",
    "        losses.append(loss)\n",
    "        if epoch==num_epochs-1:\n",
    "            with torch.no_grad():\n",
    "                predictive = Predictive(sgphomo.model, guide=guide, num_samples=1000,\n",
    "                return_sites=(\"f\", \"g\", \"_RETURN\"))\n",
    "                samples = predictive(X1)\n",
    "                f_samples = samples[\"f\"]\n",
    "                f_mean = f_samples.mean(dim=0)\n",
    "                f_mean = sgphomo.likelihood.response_function(f_mean)\n",
    "                #f_std = sgphomo_cens.likelihood.variance.sqrt().item()\n",
    "                f_std = f_mean.sqrt().detach().numpy()\n",
    "                f_025 = np.quantile(a=f_samples.detach().numpy(), q=0.025, axis=0)\n",
    "                f_975 = np.quantile(a=f_samples.detach().numpy(), q=0.975, axis=0)\n",
    "                fig = plt.figure(figsize=(14,10))\n",
    "                #fig.add_subplot(121)\n",
    "                plt.plot(X1.numpy(), y_sc.numpy(), linestyle=\"--\", color=\"black\",label='True function')\n",
    "                plt.plot(X1.detach().numpy(), f_mean.detach().numpy(), color=\"black\",label='Estimated function')\n",
    "                plt.fill_between(X1.detach().numpy(), f_mean.detach().numpy() - 1.96*f_std, f_mean.detach().numpy() + 1.96*f_std, alpha=0.3,label='Mean \\u00B1 1.96*std.dev')\n",
    "                plt.fill_between(X1.detach().numpy(), f_025, f_975, alpha=0.3,label='5%/95% Confidence interval')\n",
    "                plt.scatter(X1.numpy()[censoring==1].reshape(-1,1), y=Y1_cens_sc.numpy()[censoring==1].reshape(-1,1), marker=\"x\", label=\"Censored Observations\", color='#348ABD')\n",
    "                plt.scatter(X1.numpy()[censoring==0].reshape(-1,1), y=Y1_cens_sc.numpy()[censoring==0].reshape(-1,1), marker=\"o\", label=\"Non-Censored Observations\", color='#348ABD')\n",
    "                plt.legend(prop={'size': 14})\n",
    "                plt.ylabel('y')\n",
    "                plt.xlabel('x')\n",
    "                plt.savefig('Experiments/Synthetic/Pois/Pois_Synthetic_{}.png'.format(intens))\n",
    "                \n",
    "    fig1 = plt.figure(figsize=(8,6))\n",
    "    plt.plot(losses,label='Loss')\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.ylabel('ELBO')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.savefig('Experiments/Synthetic/Pois/Pois_Synthetic_Loss_{}.png'.format(intens)) \n",
    "    print(\"RMSE Homosced: \",sqrt(mean_squared_error(y_sc, f_mean.detach().numpy())))\n",
    "    print(\"NLPD homosced: \",-(1/len(Y1)*sgphomo.likelihood.y_dist.log_prob(y_sc).sum().item()))\n",
    "    \n",
    "    \n",
    "    print('Running Censored Poisson with intensity: {}'.format(intens))\n",
    "    pyro.clear_param_store()\n",
    "    kern = gp.kernels.RBF(input_dim=1,active_dims=[0],lengthscale=torch.tensor(1.),variance=torch.tensor(1.))\n",
    "    kern1 = gp.kernels.Linear(input_dim=1,active_dims=[0],variance=torch.tensor(1.))\n",
    "    full_k = gp.kernels.Sum(kern,kern1)\n",
    "    response_f = log_error_f\n",
    "    like_cens = Censored_Poisson(censoring=censoring,response_function=response_f)\n",
    "    sgphomo_cens = VariationalPoisGP(X=X1, y=Y1_cens_sc, kernel=kern, likelihood=like_cens, mean_function=None,\n",
    "                     latent_shape=None, whiten=False,jitter=0.05)\n",
    "    guide = AutoMultivariateNormal(sgphomo_cens.model)\n",
    "    optimizer = pyro.optim.ClippedAdam({\"lr\": 0.01,\"lrd\": 0.99996})\n",
    "    svi = SVI(sgphomo_cens.model, guide, optimizer, Trace_ELBO(num_particles=10))\n",
    "    num_epochs = 8000\n",
    "    losses = []\n",
    "    pyro.clear_param_store()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        loss = svi.step(X1, Y1_cens_sc)\n",
    "        losses.append(loss)\n",
    "        if epoch==num_epochs-1:\n",
    "            with torch.no_grad():\n",
    "                predictive = Predictive(sgphomo_cens.model, guide=guide, num_samples=1000,\n",
    "                return_sites=(\"f\", \"g\", \"_RETURN\"))\n",
    "                samples = predictive(X1)\n",
    "                f_samples = samples[\"f\"]\n",
    "                f_mean = f_samples.mean(dim=0)\n",
    "                f_mean = sgphomo_cens.likelihood.response_function(f_mean)\n",
    "                f_std = f_mean.sqrt().detach().numpy()\n",
    "                f_025 = np.quantile(a=f_samples.detach().numpy(), q=0.025, axis=0)\n",
    "                f_975 = np.quantile(a=f_samples.detach().numpy(), q=0.975, axis=0)\n",
    "                fig = plt.figure(figsize=(14,10))\n",
    "                plt.plot(X1.numpy(), y_sc.numpy(), linestyle=\"--\", color=\"black\",label='True function')\n",
    "                plt.plot(X1.detach().numpy(), f_mean.detach().numpy(), color=\"black\",label='Estimated function')\n",
    "                plt.fill_between(X1.detach().numpy(), f_mean.detach().numpy() - 1.96*f_std, f_mean.detach().numpy() + 1.96*f_std, alpha=0.3,label='Mean \\u00B1 1.96*std.dev')\n",
    "                plt.fill_between(X1.detach().numpy(), f_025, f_975, alpha=0.3,label='5%/95% Confidence interval')\n",
    "                plt.scatter(X1.numpy()[censoring==1].reshape(-1,1), y=Y1_cens_sc.numpy()[censoring==1].reshape(-1,1), marker=\"x\", label=\"Censored Observations\", color='#348ABD')\n",
    "                plt.scatter(X1.numpy()[censoring==0].reshape(-1,1), y=Y1_cens_sc.numpy()[censoring==0].reshape(-1,1), marker=\"o\", label=\"Non-Censored Observations\", color='#348ABD')\n",
    "                plt.legend(prop={'size': 14})\n",
    "                plt.ylabel('y')\n",
    "                plt.xlabel('x')\n",
    "                plt.savefig('Experiments/Synthetic/CensPois/CensPois_Synthetic_{}.png'.format(intens))\n",
    "                \n",
    "    fig1 = plt.figure(figsize=(8,6))\n",
    "    plt.plot(losses,label='Loss')\n",
    "    plt.legend(prop={'size': 14})\n",
    "    plt.ylabel('ELBO')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.savefig('Experiments/Synthetic/CensPois/CensPois_Synthetic_Loss_{}.png'.format(intens))\n",
    "    \n",
    "    print(\"RMSE Homosced: \",sqrt(mean_squared_error(y_sc, f_mean.detach().numpy())))\n",
    "    print(\"NLPD homosced: \",-(1/len(Y1)*sgphomo_cens.likelihood.y_dist.log_prob(y_sc).sum().item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
